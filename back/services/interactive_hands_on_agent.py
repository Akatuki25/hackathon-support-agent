"""
InteractiveHandsOnAgent: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ã‚ºã‚ªãƒ³ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§æ®µéšçš„ã«ç”Ÿæˆã—ã€å¿…è¦ã«å¿œã˜ã¦é¸æŠè‚¢ã‚’æç¤ºã™ã‚‹å¯¾è©±å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’èª¤é­”åŒ–ã—ãªãŒã‚‰æ®µéšçš„ã«ç”Ÿæˆ
- å¿…è¦ãªæ™‚ã ã‘é¸æŠè‚¢ã‚’æç¤ºï¼ˆæŠ€è¡“é¸å®šãªã©ï¼‰
- ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã€Œã§ããŸã€ã‚’å¾…ã¤MVPã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- å„ã‚¹ãƒ†ãƒƒãƒ—ã§DBä¿å­˜ï¼ˆä¸­æ–­æ™‚ã‚‚é€²æ—ã‚’ä¿æŒï¼‰
"""

import asyncio
import json
import uuid
from typing import Dict, Optional, AsyncGenerator, List, Any
from datetime import datetime
from dataclasses import dataclass, field, asdict
from enum import Enum

from sqlalchemy.orm import Session
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

from models.project_base import Task, TaskHandsOn, TaskDependency


class GenerationPhase(str, Enum):
    """ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º"""
    CONTEXT = "context"                    # ã‚¿ã‚¹ã‚¯ã®ä½ç½®ã¥ã‘èª¬æ˜
    OVERVIEW = "overview"                  # æ¦‚è¦ç”Ÿæˆ
    CHOICE_REQUIRED = "choice"             # é¸æŠãŒå¿…è¦
    WAITING_CHOICE_CONFIRM = "waiting_choice_confirm"  # é¸æŠç¢ºèªå¾…ã¡
    IMPLEMENTATION_PLANNING = "impl_planning"  # å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç”»
    IMPLEMENTATION_STEP = "impl_step"      # å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆä¸­
    WAITING_STEP_COMPLETE = "waiting_step" # ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†å¾…ã¡
    VERIFICATION = "verification"          # å‹•ä½œç¢ºèª
    COMPLETE = "complete"                  # å®Œäº†


@dataclass
class ChoiceOption:
    """é¸æŠè‚¢"""
    id: str
    label: str
    description: str
    pros: List[str] = field(default_factory=list)
    cons: List[str] = field(default_factory=list)


@dataclass
class ChoiceRequest:
    """é¸æŠè‚¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    choice_id: str
    question: str
    options: List[ChoiceOption]
    allow_custom: bool = True
    skip_allowed: bool = False
    research_hint: Optional[str] = None


@dataclass
class InputPrompt:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    prompt_id: str
    question: str
    placeholder: Optional[str] = None
    options: Optional[List[str]] = None  # ãƒœã‚¿ãƒ³é¸æŠè‚¢


@dataclass
class ImplementationStep:
    """å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—"""
    step_number: int
    title: str
    description: str
    content: str = ""
    is_completed: bool = False
    user_feedback: Optional[str] = None


@dataclass
class Decision:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …"""
    step_number: int
    description: str  # ã€ŒTypeScriptã‚’ä½¿ç”¨ã™ã‚‹ã€ãªã©
    reason: str       # æ¡ç”¨ç†ç”±


@dataclass
class SessionState:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹"""
    session_id: str
    task_id: str
    phase: GenerationPhase
    generated_content: Dict[str, str] = field(default_factory=dict)
    user_choices: Dict[str, Any] = field(default_factory=dict)
    user_inputs: Dict[str, str] = field(default_factory=dict)
    pending_choice: Optional[ChoiceRequest] = None
    pending_input: Optional[InputPrompt] = None
    # å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ç®¡ç†
    implementation_steps: List[ImplementationStep] = field(default_factory=list)
    current_step_index: int = 0
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …ï¼ˆæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ç”Ÿæˆã«åæ˜ ï¼‰
    decisions: List[Decision] = field(default_factory=list)
    # ä¿ç•™ä¸­ã®å¤‰æ›´ææ¡ˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¡ç”¨ç¢ºèªå¾…ã¡ï¼‰
    pending_decision: Optional[Dict[str, str]] = None
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


class InteractiveHandsOnAgent:
    """
    ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ³ã‚ºã‚ªãƒ³ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

    SSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§æ®µéšçš„ã«ç”Ÿæˆã—ã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªã‚’å¾…ã¤ã€‚
    å„ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†æ™‚ã«DBã«ä¿å­˜ã—ã€ä¸­æ–­ã—ã¦ã‚‚é€²æ—ã‚’ä¿æŒã™ã‚‹ã€‚
    """

    # é¸æŠãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    CHOICE_KEYWORDS = [
        "ãƒ©ã‚¤ãƒ–ãƒ©ãƒª", "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸", "ãƒ„ãƒ¼ãƒ«",
        "èªè¨¼", "DB", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ORM", "API", "çŠ¶æ…‹ç®¡ç†",
        "ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°", "CSS", "UI", "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ", "ãƒãƒƒãƒ—", "åœ°å›³",
        "é¸å®š", "é¸æŠ", "æ¯”è¼ƒ", "æ¤œè¨"
    ]

    def __init__(
        self,
        db: Session,
        task: Task,
        project_context: Dict,
        config: Optional[Dict] = None
    ):
        self.db = db
        self.task = task
        self.project_context = project_context
        self.config = config or {}

        # LLMåˆæœŸåŒ–
        self.llm = ChatGoogleGenerativeAI(
            model=self.config.get("model", "gemini-2.0-flash"),
            temperature=0.7
        )

    def _get_task_position(self) -> Dict:
        """ã‚¿ã‚¹ã‚¯ã®å…¨ä½“ã«ãŠã‘ã‚‹ä½ç½®ã¥ã‘ã‚’å–å¾—"""
        dependencies_from = self.db.query(TaskDependency).filter(
            TaskDependency.target_task_id == self.task.task_id
        ).all()

        dependencies_to = self.db.query(TaskDependency).filter(
            TaskDependency.source_task_id == self.task.task_id
        ).all()

        prev_tasks = []
        for dep in dependencies_from:
            source_task = self.db.query(Task).filter(
                Task.task_id == dep.source_task_id
            ).first()
            if source_task:
                prev_tasks.append({
                    "task_id": str(source_task.task_id),
                    "title": source_task.title,
                    "category": source_task.category
                })

        next_tasks = []
        for dep in dependencies_to:
            target_task = self.db.query(Task).filter(
                Task.task_id == dep.target_task_id
            ).first()
            if target_task:
                next_tasks.append({
                    "task_id": str(target_task.task_id),
                    "title": target_task.title,
                    "category": target_task.category
                })

        return {
            "current": {
                "task_id": str(self.task.task_id),
                "title": self.task.title,
                "category": self.task.category or "æœªåˆ†é¡"
            },
            "previous_tasks": prev_tasks,
            "next_tasks": next_tasks,
            "position_description": self._build_position_description(prev_tasks, next_tasks)
        }

    def _build_position_description(
        self,
        prev_tasks: List[Dict],
        next_tasks: List[Dict]
    ) -> str:
        """ä½ç½®ã¥ã‘ã®èª¬æ˜æ–‡ã‚’ç”Ÿæˆ"""
        parts = []

        if prev_tasks:
            prev_names = [t["title"] for t in prev_tasks[:3]]
            parts.append(f"å‰æã‚¿ã‚¹ã‚¯: {', '.join(prev_names)}")

        if next_tasks:
            next_names = [t["title"] for t in next_tasks[:3]]
            parts.append(f"æ¬¡ã®ã‚¿ã‚¹ã‚¯: {', '.join(next_names)}")

        if not parts:
            return "ã“ã®ã‚¿ã‚¹ã‚¯ã¯ç‹¬ç«‹ã—ãŸã‚¿ã‚¹ã‚¯ã§ã™ã€‚"

        return " â†’ ".join(parts)

    def _detect_choice_points(self) -> List[Dict]:
        """ã‚¿ã‚¹ã‚¯èª¬æ˜ã‹ã‚‰é¸æŠãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡º"""
        choice_points = []
        task_text = f"{self.task.title} {self.task.description or ''}"

        for keyword in self.CHOICE_KEYWORDS:
            if keyword in task_text:
                choice_type = self._get_choice_type(keyword)
                if choice_type and choice_type not in [cp["type"] for cp in choice_points]:
                    choice_points.append({
                        "type": choice_type,
                        "keyword": keyword,
                        "question": self._get_choice_question(choice_type)
                    })

        return choice_points

    def _get_choice_type(self, keyword: str) -> Optional[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é¸æŠã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        mapping = {
            "ãƒ©ã‚¤ãƒ–ãƒ©ãƒª": "library",
            "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯": "framework",
            "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸": "library",
            "ãƒ„ãƒ¼ãƒ«": "tool",
            "èªè¨¼": "auth",
            "DB": "database",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": "database",
            "ORM": "orm",
            "API": "api",
            "çŠ¶æ…‹ç®¡ç†": "state_management",
            "ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°": "styling",
            "CSS": "styling",
            "UI": "ui_library",
            "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ": "ui_library",
            "ãƒãƒƒãƒ—": "map_library",
            "åœ°å›³": "map_library",
        }
        return mapping.get(keyword)

    def _get_choice_question(self, choice_type: str) -> str:
        """é¸æŠã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè³ªå•æ–‡ã‚’ç”Ÿæˆ"""
        questions = {
            "library": "ä½¿ç”¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "framework": "ä½¿ç”¨ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "tool": "ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "auth": "èªè¨¼æ–¹å¼ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "database": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "orm": "ORMã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "api": "APIè¨­è¨ˆæ–¹å¼ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "state_management": "çŠ¶æ…‹ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "styling": "ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "ui_library": "UIãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
            "map_library": "åœ°å›³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’é¸å®šã—ã¾ã—ã‚‡ã†",
        }
        return questions.get(choice_type, "æŠ€è¡“ã‚’é¸å®šã—ã¾ã—ã‚‡ã†")

    async def _save_progress(self, session: SessionState, state: str = "generating") -> TaskHandsOn:
        """é€²æ—ã‚’DBã«ä¿å­˜ï¼ˆä¸­é–“ä¿å­˜ï¼‰"""
        existing = self.db.query(TaskHandsOn).filter(
            TaskHandsOn.task_id == self.task.task_id
        ).first()

        # å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ã‚’JSONã«å¤‰æ›
        steps_data = [
            {
                "step_number": s.step_number,
                "title": s.title,
                "description": s.description,
                "content": s.content,
                "is_completed": s.is_completed,
                "user_feedback": s.user_feedback
            }
            for s in session.implementation_steps
        ]

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´
        interactions = [
            {"type": "choice", "choice_id": k, **v}
            for k, v in session.user_choices.items()
        ]

        # æ±ºå®šäº‹é …ã‚’JSONã«å¤‰æ›
        decisions_data = [
            {
                "step_number": d.step_number,
                "description": d.description,
                "reason": d.reason
            }
            for d in session.decisions
        ]

        # ä¿ç•™ä¸­ã®å¤‰æ›´ææ¡ˆ
        pending_decision_data = session.pending_decision if session.pending_decision else None

        # ä¿ç•™ä¸­ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        pending_input_data = None
        if session.pending_input:
            pending_input_data = {
                "prompt_id": session.pending_input.prompt_id,
                "question": session.pending_input.question,
                "placeholder": session.pending_input.placeholder,
                "options": session.pending_input.options
            }

        user_interactions_data = {
            "choices": interactions,
            "inputs": session.user_inputs,
            "steps": steps_data,
            "current_step": session.current_step_index,
            "phase": session.phase.value,
            "decisions": decisions_data,
            "pending_decision": pending_decision_data,
            "pending_input": pending_input_data
        }

        if existing:
            existing.overview = session.generated_content.get("overview", "")
            existing.implementation_steps = session.generated_content.get("implementation", "")
            existing.verification = session.generated_content.get("verification", "")
            existing.technical_context = session.generated_content.get("context", "")
            existing.user_interactions = user_interactions_data
            existing.generation_mode = "interactive"
            existing.generation_state = state
            existing.session_id = session.session_id
            existing.updated_at = datetime.now()
            self.db.commit()
            return existing
        else:
            hands_on = TaskHandsOn(
                task_id=self.task.task_id,
                overview=session.generated_content.get("overview", ""),
                implementation_steps=session.generated_content.get("implementation", ""),
                verification=session.generated_content.get("verification", ""),
                technical_context=session.generated_content.get("context", ""),
                generation_model=self.config.get("model", "gemini-2.0-flash"),
                quality_score=0.8,
                generation_mode="interactive",
                generation_state=state,
                session_id=session.session_id,
                user_interactions=user_interactions_data
            )
            self.db.add(hands_on)
            self.db.commit()
            self.db.refresh(hands_on)
            return hands_on

    async def _generate_choice_options(
        self,
        choice_type: str,
        choice_question: str
    ) -> ChoiceRequest:
        """é¸æŠè‚¢ã‚’AIã§ç”Ÿæˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã§{choice_question}ã€‚
ä¸»è¦ãªé¸æŠè‚¢ã‚’3ã¤ç¨‹åº¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}
- ã‚«ãƒ†ã‚´ãƒª: {self.task.category or 'æœªåˆ†é¡'}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {self.project_context.get('framework', 'æœªè¨­å®š')}

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
  "options": [
    {{
      "id": "option1",
      "label": "é¸æŠè‚¢å",
      "description": "ç°¡æ½”ãªèª¬æ˜ï¼ˆ1è¡Œï¼‰",
      "pros": ["ãƒ¡ãƒªãƒƒãƒˆ1", "ãƒ¡ãƒªãƒƒãƒˆ2"],
      "cons": ["ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ1"]
    }}
  ],
  "research_hint": "èª¿ã¹ã‚‹éš›ã®ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰"
}}
"""

        response = await self.llm.ainvoke([
            SystemMessage(content="ã‚ãªãŸã¯æŠ€è¡“é¸å®šã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ])

        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            data = json.loads(content.strip())

            options = [
                ChoiceOption(
                    id=opt["id"],
                    label=opt["label"],
                    description=opt["description"],
                    pros=opt.get("pros", []),
                    cons=opt.get("cons", [])
                )
                for opt in data.get("options", [])
            ]

            return ChoiceRequest(
                choice_id=f"choice_{choice_type}_{uuid.uuid4().hex[:8]}",
                question=choice_question,
                options=options,
                allow_custom=True,
                skip_allowed=True,
                research_hint=data.get("research_hint")
            )
        except (json.JSONDecodeError, KeyError):
            return ChoiceRequest(
                choice_id=f"choice_{choice_type}_{uuid.uuid4().hex[:8]}",
                question=choice_question,
                options=[
                    ChoiceOption(
                        id="custom",
                        label="è‡ªåˆ†ã§èª¿ã¹ã¦æ±ºã‚ã‚‹",
                        description="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„è¨˜äº‹ã‚’å‚è€ƒã«è‡ªåˆ†ã§é¸å®šã—ã¾ã™"
                    )
                ],
                allow_custom=True,
                skip_allowed=True,
                research_hint="å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„æ¯”è¼ƒè¨˜äº‹ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„"
            )

    async def _generate_implementation_plan(
        self,
        user_choices: Dict[str, Any]
    ) -> List[ImplementationStep]:
        """MVPã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨ˆç”»"""
        choices_text = ""
        if user_choices:
            for choice_id, choice_data in user_choices.items():
                choices_text += f"- é¸æŠ: {choice_data.get('selected', 'ãªã—')}\n"

        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’MVPã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ®µéšçš„ã«å®Ÿè£…ã™ã‚‹è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}
{choices_text}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {self.project_context.get('framework', 'æœªè¨­å®š')}

## è¨ˆç”»ã®ãƒ«ãƒ¼ãƒ«
1. æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¯å¿…ãšã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãƒ»åˆæœŸè¨­å®šã€
2. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ŒåŸºæœ¬çš„ãªå‹•ä½œç¢ºèªãŒã§ãã‚‹æœ€å°æ§‹æˆã€
3. ãã®å¾Œã€ã‚³ã‚¢æ©Ÿèƒ½ã‚’æ®µéšçš„ã«è¿½åŠ 
4. å„ã‚¹ãƒ†ãƒƒãƒ—ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œç¢ºèªã§ãã‚‹å˜ä½ã«ã™ã‚‹
5. ã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯3ã€œ5å€‹ç¨‹åº¦

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
  "steps": [
    {{
      "step_number": 1,
      "title": "ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¤ãƒˆãƒ«",
      "description": "ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ä½•ã‚’ã™ã‚‹ã‹ï¼ˆ1-2æ–‡ï¼‰"
    }}
  ]
}}
"""

        response = await self.llm.ainvoke([
            SystemMessage(content="ã‚ãªãŸã¯MVPé–‹ç™ºã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ])

        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            data = json.loads(content.strip())

            return [
                ImplementationStep(
                    step_number=s["step_number"],
                    title=s["title"],
                    description=s["description"]
                )
                for s in data.get("steps", [])
            ]
        except (json.JSONDecodeError, KeyError):
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ãƒ†ãƒƒãƒ—
            return [
                ImplementationStep(1, "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸè¨­å®š", "å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™"),
                ImplementationStep(2, "åŸºæœ¬å®Ÿè£…", "æœ€å°é™ã®å‹•ä½œã™ã‚‹å®Ÿè£…ã‚’ä½œæˆã—ã¾ã™"),
                ImplementationStep(3, "æ©Ÿèƒ½è¿½åŠ ", "ã‚³ã‚¢æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™"),
            ]

    async def _generate_step_content(
        self,
        step: ImplementationStep,
        user_choices: Dict[str, Any],
        previous_steps: List[ImplementationStep],
        decisions: List[Decision] = None
    ) -> AsyncGenerator[str, None]:
        """ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè£…å†…å®¹ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        choices_text = ""
        if user_choices:
            for choice_id, choice_data in user_choices.items():
                choices_text += f"- é¸æŠ: {choice_data.get('selected', 'ãªã—')}\n"

        prev_steps_text = ""
        if previous_steps:
            prev_steps_text = "\n## å®Œäº†æ¸ˆã¿ã‚¹ãƒ†ãƒƒãƒ—\n"
            for ps in previous_steps:
                prev_steps_text += f"- ã‚¹ãƒ†ãƒƒãƒ—{ps.step_number}: {ps.title} âœ“\n"

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …
        decisions_context = ""
        if decisions:
            decisions_context = "\n## ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …ï¼ˆå¿…ãšåæ˜ ã—ã¦ãã ã•ã„ï¼‰\n"
            for d in decisions:
                decisions_context += f"- **{d.description}**ï¼ˆã‚¹ãƒ†ãƒƒãƒ—{d.step_number}ã§æ±ºå®šï¼‰\n"

        prompt = f"""
ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è©³ç´°ãªå®Ÿè£…æ‰‹é †ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}
{choices_text}
{prev_steps_text}
{decisions_context}

## ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
- ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}
- ç›®çš„: {step.description}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {self.project_context.get('framework', 'æœªè¨­å®š')}
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : {self.project_context.get('directory_info', 'æœªè¨­å®š')[:500]}

## é‡è¦ãªæ³¨æ„äº‹é …
- ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …ã€ã¯å¿…ãšåæ˜ ã—ã¦ãã ã•ã„
- ä¾‹: TypeScriptã‚’ä½¿ã†ã¨æ±ºã¾ã£ã¦ã„ãŸã‚‰ã€å¿…ãšTypeScriptã§ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æ›¸ã„ã¦ãã ã•ã„
- ä¾‹: ç‰¹å®šã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã¨æ±ºã¾ã£ã¦ã„ãŸã‚‰ã€å¿…ãšãã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ãã ã•ã„

## å‡ºåŠ›å½¢å¼
Markdownå½¢å¼ã§ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š

### ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}

#### ç›®çš„

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç›®çš„ã‚’1-2æ–‡ã§èª¬æ˜

#### å®Ÿè£…æ‰‹é †

1. æœ€åˆã«ã‚„ã‚‹ã“ã¨

```è¨€èª
ã‚³ãƒ¼ãƒ‰ä¾‹
```

2. æ¬¡ã«ã‚„ã‚‹ã“ã¨

```è¨€èª
ã‚³ãƒ¼ãƒ‰ä¾‹
```

#### å‹•ä½œç¢ºèª

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå®Œäº†ã—ãŸã‹ç¢ºèªã™ã‚‹æ–¹æ³•

---

**é‡è¦ãªæ›¸å¼ãƒ«ãƒ¼ãƒ«:**
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–“ã«ã¯å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- è¦‹å‡ºã—ï¼ˆ###, ####ï¼‰ã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- ç®‡æ¡æ›¸ãã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
"""

        async for chunk in self.llm.astream([
            SystemMessage(content="ã‚ãªãŸã¯ä¸å¯§ãªé–‹ç™ºã‚¬ã‚¤ãƒ‰ã‚’ä½œæˆã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¡ç”¨ã—ãŸæ±ºå®šäº‹é …ï¼ˆè¨€èªã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç­‰ï¼‰ã¯å¿…ãšåæ˜ ã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ]):
            if chunk.content:
                yield chunk.content

    async def generate_stream(
        self,
        session: SessionState
    ) -> AsyncGenerator[Dict, None]:
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒãƒ³ã‚ºã‚ªãƒ³ã‚’ç”Ÿæˆ

        Yields:
            SSEã‚¤ãƒ™ãƒ³ãƒˆè¾æ›¸
        """
        try:
            # Phase 1: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¹ã‚¯ã®ä½ç½®ã¥ã‘ï¼‰
            if session.phase == GenerationPhase.CONTEXT:
                position = self._get_task_position()

                yield {
                    "type": "context",
                    "position": position["position_description"],
                    "dependencies": [t["title"] for t in position["previous_tasks"]],
                    "dependents": [t["title"] for t in position["next_tasks"]]
                }

                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ã‚’é€šçŸ¥
                yield {"type": "section_start", "section": "context"}

                context_text = self._build_context_text(position)
                for chunk in self._chunk_text(context_text):
                    yield {"type": "chunk", "content": chunk}
                    await asyncio.sleep(0.02)

                session.generated_content["context"] = context_text
                yield {"type": "section_complete", "section": "context"}
                session.phase = GenerationPhase.OVERVIEW

                # ä¸­é–“ä¿å­˜
                await self._save_progress(session, "generating")
                yield {"type": "progress_saved", "phase": "context"}

            # Phase 2: æ¦‚è¦ç”Ÿæˆ
            if session.phase == GenerationPhase.OVERVIEW:
                yield {"type": "section_start", "section": "overview"}

                async for chunk in self._stream_overview():
                    yield {"type": "chunk", "content": chunk}
                    session.generated_content["overview"] = session.generated_content.get("overview", "") + chunk

                yield {"type": "section_complete", "section": "overview"}

                # ä¸­é–“ä¿å­˜
                await self._save_progress(session, "generating")
                yield {"type": "progress_saved", "phase": "overview"}

                # é¸æŠãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
                choice_points = self._detect_choice_points()
                if choice_points and not session.user_choices:
                    first_choice = choice_points[0]
                    choice_request = await self._generate_choice_options(
                        first_choice["type"],
                        first_choice["question"]
                    )
                    session.pending_choice = choice_request
                    session.phase = GenerationPhase.CHOICE_REQUIRED

                    # ä¸­é–“ä¿å­˜
                    await self._save_progress(session, "waiting_input")

                    yield {
                        "type": "choice_required",
                        "choice": {
                            "choice_id": choice_request.choice_id,
                            "question": choice_request.question,
                            "options": [
                                {
                                    "id": opt.id,
                                    "label": opt.label,
                                    "description": opt.description,
                                    "pros": opt.pros,
                                    "cons": opt.cons
                                }
                                for opt in choice_request.options
                            ],
                            "allow_custom": choice_request.allow_custom,
                            "skip_allowed": choice_request.skip_allowed,
                            "research_hint": choice_request.research_hint
                        }
                    }
                    return
                else:
                    session.phase = GenerationPhase.IMPLEMENTATION_PLANNING

            # Phase 3: å®Ÿè£…è¨ˆç”»
            if session.phase == GenerationPhase.IMPLEMENTATION_PLANNING:
                yield {"type": "section_start", "section": "planning"}
                yield {"type": "chunk", "content": "\n\n### å®Ÿè£…è¨ˆç”»\n\nMVPã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ®µéšçš„ã«å®Ÿè£…ã—ã¦ã„ãã¾ã™ã€‚\n\n"}

                # ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨ˆç”»
                session.implementation_steps = await self._generate_implementation_plan(session.user_choices)

                # ã‚¹ãƒ†ãƒƒãƒ—ä¸€è¦§ã‚’è¡¨ç¤º
                steps_overview = ""
                for step in session.implementation_steps:
                    steps_overview += f"**ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}**: {step.title}\n"
                    steps_overview += f"  - {step.description}\n\n"

                for chunk in self._chunk_text(steps_overview):
                    yield {"type": "chunk", "content": chunk}
                    await asyncio.sleep(0.02)

                yield {"type": "section_complete", "section": "planning"}

                # ä¸­é–“ä¿å­˜
                await self._save_progress(session, "generating")

                session.current_step_index = 0
                session.phase = GenerationPhase.IMPLEMENTATION_STEP

            # Phase 4: å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ç”Ÿæˆâ†’ç¢ºèªå¾…ã¡ï¼‰
            if session.phase == GenerationPhase.IMPLEMENTATION_STEP:
                if session.current_step_index < len(session.implementation_steps):
                    current_step = session.implementation_steps[session.current_step_index]
                    previous_steps = session.implementation_steps[:session.current_step_index]

                    yield {
                        "type": "step_start",
                        "step_number": current_step.step_number,
                        "step_title": current_step.title,
                        "total_steps": len(session.implementation_steps)
                    }

                    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹ã‚’é€šçŸ¥ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãŒchunkã®è¡Œãå…ˆã‚’çŸ¥ã‚‹ãŸã‚ï¼‰
                    section_name = f"step_{current_step.step_number}"
                    yield {"type": "section_start", "section": section_name}

                    # ã‚¹ãƒ†ãƒƒãƒ—å†…å®¹ã‚’ç”Ÿæˆï¼ˆæ±ºå®šäº‹é …ã‚’åæ˜ ï¼‰
                    step_content = ""
                    async for chunk in self._generate_step_content(
                        current_step,
                        session.user_choices,
                        previous_steps,
                        session.decisions
                    ):
                        yield {"type": "chunk", "content": chunk}
                        step_content += chunk

                    current_step.content = step_content

                    # å®Ÿè£…å†…å®¹ã‚’ç´¯ç©
                    session.generated_content["implementation"] = session.generated_content.get("implementation", "") + "\n\n" + step_content

                    yield {"type": "section_complete", "section": section_name}
                    yield {"type": "step_complete", "step_number": current_step.step_number}

                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªå¾…ã¡çŠ¶æ…‹ã‚’è¨­å®š
                    session.phase = GenerationPhase.WAITING_STEP_COMPLETE
                    session.pending_input = InputPrompt(
                        prompt_id=f"step_{current_step.step_number}_complete",
                        question=f"ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã€Œ{current_step.title}ã€ã¯å®Œäº†ã—ã¾ã—ãŸã‹ï¼Ÿ",
                        placeholder="ã§ããŸ / è³ªå•ãŒã‚ã‚‹",
                        options=["ã§ããŸ", "è³ªå•ãŒã‚ã‚‹", "ã‚¹ã‚­ãƒƒãƒ—"]
                    )

                    # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                    await self._save_progress(session, "waiting_input")
                    yield {"type": "progress_saved", "phase": f"step_{current_step.step_number}"}

                    yield {
                        "type": "step_confirmation_required",
                        "prompt": {
                            "prompt_id": session.pending_input.prompt_id,
                            "question": session.pending_input.question,
                            "options": session.pending_input.options
                        }
                    }
                    return
                else:
                    # å…¨ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†
                    session.phase = GenerationPhase.VERIFICATION

            # Phase 5: å‹•ä½œç¢ºèª
            if session.phase == GenerationPhase.VERIFICATION:
                yield {"type": "section_start", "section": "verification"}

                async for chunk in self._stream_verification():
                    yield {"type": "chunk", "content": chunk}
                    session.generated_content["verification"] = session.generated_content.get("verification", "") + chunk

                yield {"type": "section_complete", "section": "verification"}

                # ä¸­é–“ä¿å­˜
                await self._save_progress(session, "generating")

                session.phase = GenerationPhase.COMPLETE

            # Phase 6: å®Œäº†
            if session.phase == GenerationPhase.COMPLETE:
                hands_on = await self._save_progress(session, "completed")

                yield {
                    "type": "done",
                    "hands_on_id": str(hands_on.hands_on_id),
                    "session_id": session.session_id
                }

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚é€²æ—ã‚’ä¿å­˜
            try:
                await self._save_progress(session, "generating")
            except:
                pass
            yield {"type": "error", "message": str(e)}

    def _build_context_text(self, position: Dict) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        parts = [f"## {self.task.title}\n\n"]

        if self.task.description:
            parts.append(f"{self.task.description}\n\n")

        parts.append(f"### ã‚¿ã‚¹ã‚¯ã®ä½ç½®ã¥ã‘\n\n")
        parts.append(f"{position['position_description']}\n\n")

        if position["previous_tasks"]:
            parts.append("**å‰æã¨ãªã‚‹ã‚¿ã‚¹ã‚¯:**\n\n")
            for task in position["previous_tasks"][:3]:
                parts.append(f"- {task['title']}\n")
            parts.append("\n")

        if position["next_tasks"]:
            parts.append("**ã“ã®ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«å®Ÿè£…ã§ãã‚‹ã‚¿ã‚¹ã‚¯:**\n\n")
            for task in position["next_tasks"][:3]:
                parts.append(f"- {task['title']}\n")
            parts.append("\n")

        return "".join(parts)

    def _chunk_text(self, text: str, chunk_size: int = 5) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ï¼‰"""
        words = list(text)
        return [
            "".join(words[i:i + chunk_size])
            for i in range(0, len(words), chunk_size)
        ]

    async def _stream_overview(self) -> AsyncGenerator[str, None]:
        """æ¦‚è¦ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®æ¦‚è¦ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
ã“ã®ã‚¿ã‚¹ã‚¯ã§ä½•ã‚’å®Ÿè£…ã™ã‚‹ã‹ã€ãªãœå¿…è¦ã‹ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}
- ã‚«ãƒ†ã‚´ãƒª: {self.task.category or 'æœªåˆ†é¡'}
- å„ªå…ˆåº¦: {self.task.priority or 'Must'}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {self.project_context.get('framework', 'æœªè¨­å®š')}

## å‡ºåŠ›å½¢å¼
Markdownå½¢å¼ã§ã€200-300æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæ›¸å¼ãƒ«ãƒ¼ãƒ«:**
- æ®µè½é–“ã«ã¯å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- è¦‹å‡ºã—ï¼ˆ##, ###ï¼‰ã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- ç®‡æ¡æ›¸ãã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
"""

        async for chunk in self.llm.astream([
            SystemMessage(content="ã‚ãªãŸã¯é–‹ç™ºã‚¬ã‚¤ãƒ‰ã‚’ä½œæˆã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚"),
            HumanMessage(content=prompt)
        ]):
            if chunk.content:
                yield chunk.content

    async def _stream_verification(self) -> AsyncGenerator[str, None]:
        """å‹•ä½œç¢ºèªæ‰‹é †ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ"""
        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯å…¨ä½“ã®æœ€çµ‚å‹•ä½œç¢ºèªæ–¹æ³•ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}

## å‡ºåŠ›å½¢å¼
Markdownå½¢å¼ã§ã€ä»¥ä¸‹ã®æ§‹æˆã§èª¬æ˜ã—ã¦ãã ã•ã„ï¼š

### æœ€çµ‚å‹•ä½œç¢ºèª

1. ã€‡ã€‡ã‚’ç¢ºèª
2. ã€‡ã€‡ã‚’å®Ÿè¡Œ
3. æœŸå¾…ã•ã‚Œã‚‹çµæœ: ã€‡ã€‡

### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•

- **ã‚¨ãƒ©ãƒ¼1**: ã€‡ã€‡

  - åŸå› : ã€‡ã€‡
  - å¯¾å‡¦æ³•: ã€‡ã€‡

**é‡è¦ãªæ›¸å¼ãƒ«ãƒ¼ãƒ«:**
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ»æ®µè½ã®é–“ã«ã¯å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- è¦‹å‡ºã—ï¼ˆ###ï¼‰ã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- ç®‡æ¡æ›¸ãã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
"""

        async for chunk in self.llm.astream([
            SystemMessage(content="ã‚ãªãŸã¯é–‹ç™ºã‚¬ã‚¤ãƒ‰ã‚’ä½œæˆã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚"),
            HumanMessage(content=prompt)
        ]):
            if chunk.content:
                yield chunk.content

    async def _generate_pros_cons_analysis(
        self,
        choice_type: str,
        user_choice: str,
        user_note: Optional[str] = None
    ) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠã«å¯¾ã™ã‚‹ãƒ¡ãƒªãƒ‡ãƒ¡åˆ†æã‚’ç”Ÿæˆ"""
        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»¥ä¸‹ã®é¸æŠã‚’ã—ã¾ã—ãŸã€‚ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’ç°¡æ½”ã«åˆ†æã—ã¦ãã ã•ã„ã€‚

## é¸æŠå†…å®¹
- é¸æŠ: {user_choice}
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒ¢: {user_note or 'ãªã—'}

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task.title}
- èª¬æ˜: {self.task.description or 'ãªã—'}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®å½¢å¼ã§ã€ç°¡æ½”ã«ï¼ˆå…¨ä½“ã§200æ–‡å­—ç¨‹åº¦ï¼‰åˆ†æã—ã¦ãã ã•ã„ï¼š

**{user_choice}ã®ç‰¹å¾´:**

âœ“ ãƒ¡ãƒªãƒƒãƒˆ1
âœ“ ãƒ¡ãƒªãƒƒãƒˆ2

â–³ æ³¨æ„ç‚¹1
â–³ æ³¨æ„ç‚¹2

ã“ã®é¸æŠã§é€²ã‚ã¾ã™ã‹ï¼Ÿ
"""

        response = await self.llm.ainvoke([
            SystemMessage(content="ã‚ãªãŸã¯æŠ€è¡“é¸å®šã®ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ç°¡æ½”ã«åˆ†æã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ])

        return response.content

    async def handle_user_response(
        self,
        session: SessionState,
        response_type: str,
        choice_id: Optional[str] = None,
        selected: Optional[str] = None,
        user_input: Optional[str] = None,
        user_note: Optional[str] = None
    ) -> AsyncGenerator[Dict, None]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿œç­”ã‚’å‡¦ç†ã—ã¦ç”Ÿæˆã‚’ç¶™ç¶š"""
        session.updated_at = datetime.now()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¿œç­”ã‚’ã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦é€šçŸ¥
        yield {
            "type": "user_response",
            "response_type": response_type,
            "choice_id": choice_id,
            "selected": selected,
            "user_input": user_input,
            "user_note": user_note
        }

        if response_type == "choice":
            # é¸æŠã‚’è¨˜éŒ²
            session.user_choices[choice_id] = {
                "selected": selected or user_input,
                "note": user_note
            }

            # ãƒ¡ãƒªãƒ‡ãƒ¡åˆ†æã‚’ç”Ÿæˆ
            if session.pending_choice:
                analysis = await self._generate_pros_cons_analysis(
                    session.pending_choice.choice_id,
                    selected or user_input,
                    user_note
                )

                yield {"type": "section_start", "section": "analysis"}
                for chunk in self._chunk_text(analysis):
                    yield {"type": "chunk", "content": chunk}
                    await asyncio.sleep(0.02)
                yield {"type": "section_complete", "section": "analysis"}

                # ç¢ºèªã‚’æ±‚ã‚ã‚‹
                session.pending_input = InputPrompt(
                    prompt_id="confirm_choice",
                    question="ã“ã®é¸æŠã§é€²ã‚ã¾ã™ã‹ï¼Ÿ",
                    options=["OK", "åˆ¥ã®é¸æŠè‚¢ã‚’æ¤œè¨"]
                )
                session.phase = GenerationPhase.WAITING_CHOICE_CONFIRM

                # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                await self._save_progress(session, "waiting_input")

                yield {
                    "type": "user_input_required",
                    "prompt": {
                        "prompt_id": "confirm_choice",
                        "question": "ã“ã®é¸æŠã§é€²ã‚ã¾ã™ã‹ï¼Ÿ",
                        "options": ["OK", "åˆ¥ã®é¸æŠè‚¢ã‚’æ¤œè¨"]
                    }
                }
                return

        elif response_type == "input":
            # é¸æŠç¢ºèªã¸ã®å¿œç­”
            if session.phase == GenerationPhase.WAITING_CHOICE_CONFIRM:
                if user_input and user_input.upper() in ["OK", "ã¯ã„", "YES", "é€²ã‚ã‚‹"]:
                    session.phase = GenerationPhase.IMPLEMENTATION_PLANNING
                    session.pending_choice = None
                    session.pending_input = None
                else:
                    session.phase = GenerationPhase.OVERVIEW
                    session.pending_input = None
                    session.user_choices = {}

            # ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†ç¢ºèªã¸ã®å¿œç­”
            elif session.phase == GenerationPhase.WAITING_STEP_COMPLETE:
                current_step = session.implementation_steps[session.current_step_index]

                if user_input in ["ã§ããŸ", "å®Œäº†", "done"]:
                    current_step.is_completed = True
                    current_step.user_feedback = "completed"
                    session.current_step_index += 1
                    session.phase = GenerationPhase.IMPLEMENTATION_STEP
                    session.pending_input = None

                elif user_input == "ã‚¹ã‚­ãƒƒãƒ—":
                    current_step.is_completed = True
                    current_step.user_feedback = "skipped"
                    session.current_step_index += 1
                    session.phase = GenerationPhase.IMPLEMENTATION_STEP
                    session.pending_input = None

                elif user_input in ["è³ªå•ãŒã‚ã‚‹", "ã¾ã è³ªå•ãŒã‚ã‚‹"]:
                    # è³ªå•å…¥åŠ›ã‚’æ±‚ã‚ã‚‹
                    session.pending_input = InputPrompt(
                        prompt_id=f"question_step_{current_step.step_number}",
                        question=f"ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã€Œ{current_step.title}ã€ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„",
                        placeholder="ã‚ã‹ã‚‰ãªã„ã“ã¨ã‚„è©°ã¾ã£ã¦ã„ã‚‹ç‚¹ã‚’å…¥åŠ›..."
                    )
                    # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                    await self._save_progress(session, "waiting_input")
                    yield {
                        "type": "user_input_required",
                        "prompt": {
                            "prompt_id": session.pending_input.prompt_id,
                            "question": session.pending_input.question,
                            "placeholder": session.pending_input.placeholder
                        }
                    }
                    return

                elif user_input == "æ¡ç”¨ã™ã‚‹" and session.pending_decision:
                    # å¤‰æ›´ææ¡ˆã‚’æ¡ç”¨
                    new_decision = Decision(
                        step_number=current_step.step_number,
                        description=session.pending_decision["proposal"],
                        reason=session.pending_decision["reason"]
                    )
                    session.decisions.append(new_decision)
                    yield {"type": "chunk", "content": f"\n\nâœ“ **æ±ºå®šäº‹é …ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ:** {session.pending_decision['proposal']}\n\n"}
                    session.pending_decision = None

                    # æ±ºå®šã‚’åæ˜ ã—ã¦ã‚¹ãƒ†ãƒƒãƒ—å†…å®¹ã‚’å†ç”Ÿæˆ
                    yield {"type": "chunk", "content": f"---\n\n**æ±ºå®šã‚’åæ˜ ã—ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã®å†…å®¹ã‚’æ›´æ–°ã—ã¾ã™...**\n\n"}
                    yield {"type": "section_start", "section": f"step_{current_step.step_number}_updated"}

                    previous_steps = [s for s in session.implementation_steps[:session.current_step_index]]
                    updated_content = ""
                    async for chunk in self._generate_step_content(
                        current_step,
                        session.user_choices,
                        previous_steps,
                        session.decisions
                    ):
                        yield {"type": "chunk", "content": chunk}
                        updated_content += chunk

                    current_step.content = updated_content
                    yield {"type": "section_complete", "section": f"step_{current_step.step_number}_updated"}

                    # å†åº¦ã‚¹ãƒ†ãƒƒãƒ—ç¢ºèªã‚’æ±‚ã‚ã‚‹
                    session.pending_input = InputPrompt(
                        prompt_id=f"step_{current_step.step_number}_complete",
                        question=f"ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã€Œ{current_step.title}ã€ã®æ›´æ–°å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚å®Œäº†ã—ã¾ã—ãŸã‹ï¼Ÿ",
                        options=["ã§ããŸ", "ã¾ã è³ªå•ãŒã‚ã‚‹", "ã‚¹ã‚­ãƒƒãƒ—"]
                    )
                    # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                    await self._save_progress(session, "waiting_input")
                    yield {
                        "type": "step_confirmation_required",
                        "prompt": {
                            "prompt_id": session.pending_input.prompt_id,
                            "question": session.pending_input.question,
                            "options": session.pending_input.options
                        }
                    }
                    return

                elif user_input == "æ¡ç”¨ã—ãªã„" and session.pending_decision:
                    # å¤‰æ›´ææ¡ˆã‚’æ¡ç”¨ã—ãªã„
                    yield {"type": "chunk", "content": "\n\nç¾çŠ¶ã®ã¾ã¾é€²ã‚ã¾ã™ã€‚\n\n"}
                    session.pending_decision = None

                    # å†åº¦ã‚¹ãƒ†ãƒƒãƒ—ç¢ºèªã‚’æ±‚ã‚ã‚‹
                    session.pending_input = InputPrompt(
                        prompt_id=f"step_{current_step.step_number}_complete",
                        question=f"ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã€Œ{current_step.title}ã€ã¯å®Œäº†ã—ã¾ã—ãŸã‹ï¼Ÿ",
                        options=["ã§ããŸ", "ã¾ã è³ªå•ãŒã‚ã‚‹", "ã‚¹ã‚­ãƒƒãƒ—"]
                    )
                    # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                    await self._save_progress(session, "waiting_input")
                    yield {
                        "type": "step_confirmation_required",
                        "prompt": {
                            "prompt_id": session.pending_input.prompt_id,
                            "question": session.pending_input.question,
                            "options": session.pending_input.options
                        }
                    }
                    return

                else:
                    # ãã®ä»–ã®å…¥åŠ›ã¯è³ªå•/ææ¡ˆã¨ã—ã¦åˆ†æ
                    current_step.user_feedback = user_input

                    # å¤‰æ›´ææ¡ˆã‹ã©ã†ã‹ã‚’åˆ†æ
                    decision_proposal = await self._analyze_question_for_decision(user_input, current_step)

                    if decision_proposal:
                        # å¤‰æ›´ææ¡ˆãŒæ¤œå‡ºã•ã‚ŒãŸ â†’ ãƒ¡ãƒªãƒ‡ãƒ¡åˆ†æã—ã¦ã‹ã‚‰æ¡ç”¨ç¢ºèª
                        session.pending_decision = decision_proposal
                        yield {"type": "section_start", "section": "proposal"}
                        yield {"type": "chunk", "content": f"\n\n**å¤‰æ›´ææ¡ˆã‚’æ¤œå‡ºã—ã¾ã—ãŸ:**\n\n"}
                        yield {"type": "chunk", "content": f"ğŸ“ **{decision_proposal['proposal']}**\n\n"}

                        # ãƒ¡ãƒªãƒ‡ãƒ¡åˆ†æã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
                        yield {"type": "chunk", "content": "---\n\n"}
                        async for chunk in self._stream_pros_cons_analysis(
                            decision_proposal['proposal'],
                            current_step
                        ):
                            yield {"type": "chunk", "content": chunk}

                        yield {"type": "chunk", "content": "\n\n---\n\n"}
                        yield {"type": "section_complete", "section": "proposal"}

                        session.pending_input = InputPrompt(
                            prompt_id=f"decision_confirm_{current_step.step_number}",
                            question="ã“ã®å¤‰æ›´ã‚’æ¡ç”¨ã—ã¾ã™ã‹ï¼Ÿ",
                            options=["æ¡ç”¨ã™ã‚‹", "æ¡ç”¨ã—ãªã„"]
                        )
                        # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                        await self._save_progress(session, "waiting_input")
                        yield {
                            "type": "user_input_required",
                            "prompt": {
                                "prompt_id": session.pending_input.prompt_id,
                                "question": session.pending_input.question,
                                "options": session.pending_input.options
                            }
                        }
                        return
                    else:
                        # å˜ç´”ãªè³ªå• â†’ å›ç­”ã®ã¿
                        yield {"type": "section_start", "section": "answer"}
                        async for chunk in self._stream_answer_question(user_input, current_step, session.decisions):
                            yield {"type": "chunk", "content": chunk}
                        yield {"type": "section_complete", "section": "answer"}

                        # å†åº¦ã‚¹ãƒ†ãƒƒãƒ—ç¢ºèªã‚’æ±‚ã‚ã‚‹
                        session.pending_input = InputPrompt(
                            prompt_id=f"step_{current_step.step_number}_complete",
                            question=f"è³ªå•ã«å›ç­”ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒƒãƒ—{current_step.step_number}ã€Œ{current_step.title}ã€ã¯å®Œäº†ã—ã¾ã—ãŸã‹ï¼Ÿ",
                            options=["ã§ããŸ", "ã¾ã è³ªå•ãŒã‚ã‚‹", "ã‚¹ã‚­ãƒƒãƒ—"]
                        )
                        # pending_inputã‚’è¨­å®šã—ãŸå¾Œã«ä¿å­˜
                        await self._save_progress(session, "waiting_input")
                        yield {
                            "type": "step_confirmation_required",
                            "prompt": {
                                "prompt_id": session.pending_input.prompt_id,
                                "question": session.pending_input.question,
                                "options": session.pending_input.options
                            }
                        }
                        return

        elif response_type == "skip":
            if session.phase == GenerationPhase.CHOICE_REQUIRED:
                session.phase = GenerationPhase.IMPLEMENTATION_PLANNING
                session.pending_choice = None
                session.pending_input = None
            elif session.phase == GenerationPhase.WAITING_STEP_COMPLETE:
                current_step = session.implementation_steps[session.current_step_index]
                current_step.is_completed = True
                current_step.user_feedback = "skipped"
                session.current_step_index += 1
                session.phase = GenerationPhase.IMPLEMENTATION_STEP
                session.pending_input = None

        # ä¸­é–“ä¿å­˜
        await self._save_progress(session, "generating")

        # ç”Ÿæˆã‚’ç¶™ç¶š
        async for event in self.generate_stream(session):
            yield event

    async def _answer_question(self, question: str, step: ImplementationStep) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—ã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”"""
        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
- ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}
- èª¬æ˜: {step.description}

## ã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹
{step.content[:1000]}

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{question}

## å›ç­”ãƒ«ãƒ¼ãƒ«
- ç°¡æ½”ã«ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰å›ç­”
- å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ãŒã‚ã‚Œã°å«ã‚ã‚‹
- æ®µè½é–“ã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
"""

        response = await self.llm.ainvoke([
            SystemMessage(content="ã‚ãªãŸã¯ä¸å¯§ãªé–‹ç™ºã‚µãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚"),
            HumanMessage(content=prompt)
        ])

        return response.content

    async def _stream_answer_question(
        self,
        question: str,
        step: ImplementationStep,
        decisions: List[Decision] = None
    ) -> AsyncGenerator[str, None]:
        """ã‚¹ãƒ†ãƒƒãƒ—ã«é–¢ã™ã‚‹è³ªå•ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å›ç­”"""
        # æ—¢å­˜ã®æ±ºå®šäº‹é …ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
        decisions_context = ""
        if decisions:
            decisions_context = "\n## æ¡ç”¨æ¸ˆã¿ã®æ±ºå®šäº‹é …ï¼ˆã“ã‚Œã‚‰ã‚’è€ƒæ…®ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼‰\n"
            for d in decisions:
                decisions_context += f"- {d.description}\n"

        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
- ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}
- èª¬æ˜: {step.description}

## ã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹
{step.content[:1500]}
{decisions_context}

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
{question}

## å›ç­”ãƒ«ãƒ¼ãƒ«
- ã‚ã‹ã‚Šã‚„ã™ãä¸å¯§ã«å›ç­”
- å…·ä½“çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹ãŒã‚ã‚Œã°å«ã‚ã‚‹
- æ®µè½é–“ã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å‰å¾Œã«ã¯ç©ºè¡Œã‚’å…¥ã‚Œã‚‹
- æ¡ç”¨æ¸ˆã¿ã®æ±ºå®šäº‹é …ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’è€ƒæ…®ã—ã¦å›ç­”ã—ã¦ãã ã•ã„
"""

        async for chunk in self.llm.astream([
            SystemMessage(content="ã‚ãªãŸã¯ä¸å¯§ãªé–‹ç™ºã‚µãƒãƒ¼ã‚¿ãƒ¼ã§ã™ã€‚åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ]):
            if chunk.content:
                yield chunk.content

    async def _stream_pros_cons_analysis(
        self,
        proposal: str,
        step: ImplementationStep
    ) -> AsyncGenerator[str, None]:
        """å¤‰æ›´ææ¡ˆã®ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§åˆ†æ"""
        prompt = f"""
ä»¥ä¸‹ã®å¤‰æ›´ææ¡ˆã«ã¤ã„ã¦ã€ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’ç°¡æ½”ã«åˆ†æã—ã¦ãã ã•ã„ã€‚

## å¤‰æ›´ææ¡ˆ
{proposal}

## ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
- ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}
- å†…å®¹: {step.content[:500]}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯: {', '.join(self.project_context.get('tech_stack', []))}

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®å½¢å¼ã§ã€ç°¡æ½”ã«ï¼ˆå…¨ä½“ã§150-200æ–‡å­—ç¨‹åº¦ï¼‰åˆ†æã—ã¦ãã ã•ã„ï¼š

**ãƒ¡ãƒªãƒƒãƒˆ:**

âœ“ ãƒ¡ãƒªãƒƒãƒˆ1
âœ“ ãƒ¡ãƒªãƒƒãƒˆ2

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãƒ»æ³¨æ„ç‚¹:**

â–³ æ³¨æ„ç‚¹1
â–³ æ³¨æ„ç‚¹2
"""

        async for chunk in self.llm.astream([
            SystemMessage(content="æŠ€è¡“é¸å®šã®ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã¨ã—ã¦ã€ç°¡æ½”ã«ãƒ¡ãƒªãƒ‡ãƒ¡ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ]):
            if chunk.content:
                yield chunk.content

    async def _analyze_question_for_decision(
        self,
        question: str,
        step: ImplementationStep
    ) -> Optional[Dict[str, str]]:
        """
        è³ªå•ã‚’åˆ†æã—ã€å¤‰æ›´ææ¡ˆãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤æ–­ã€‚
        å«ã¾ã‚Œã¦ã„ã‚Œã°ææ¡ˆå†…å®¹ã‚’è¿”ã™ã€ãªã‘ã‚Œã°Noneã€‚
        """
        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

## ã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹
- ã‚¹ãƒ†ãƒƒãƒ—{step.step_number}: {step.title}
- å†…å®¹: {step.content[:800]}

## ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
ã€Œ{question}ã€

## åˆ†æã‚¿ã‚¹ã‚¯
ã“ã®å…¥åŠ›ãŒä»¥ä¸‹ã®ã©ã¡ã‚‰ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ï¼š

A) **å¤‰æ›´ææ¡ˆãƒ»è¦æœ›**: æŠ€è¡“é¸æŠã€è¨€èªã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãªã©ã‚’å¤‰æ›´ã—ãŸã„æ„å›³ãŒã‚ã‚‹
   ä¾‹: ã€ŒTypeScriptã®æ–¹ãŒã„ã„ã€ã€ŒReduxã˜ã‚ƒãªãã¦zustandã‚’ä½¿ã„ãŸã„ã€ã€Œã‚‚ã£ã¨ã‚·ãƒ³ãƒ—ãƒ«ã«ã§ããªã„ï¼Ÿã€

B) **å˜ç´”ãªè³ªå•**: ç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã®è³ªå•ã€ã‚¨ãƒ©ãƒ¼ã®ç›¸è«‡ãªã©
   ä¾‹: ã€Œã“ã‚Œã©ã†ã„ã†æ„å‘³ï¼Ÿã€ã€Œãªãœã“ã†ã™ã‚‹ã®ï¼Ÿã€ã€Œã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã€

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
å¤‰æ›´ææ¡ˆã®å ´åˆ:
{{"type": "decision", "proposal": "ã€‡ã€‡ã‚’ä½¿ç”¨ã™ã‚‹", "reason": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€‡ã€‡ã¨è¨€ã£ãŸãŸã‚"}}

å˜ç´”ãªè³ªå•ã®å ´åˆ:
{{"type": "question"}}
"""

        response = await self.llm.ainvoke([
            SystemMessage(content="JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"),
            HumanMessage(content=prompt)
        ])

        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            data = json.loads(content.strip())

            if data.get("type") == "decision":
                return {
                    "proposal": data.get("proposal", ""),
                    "reason": data.get("reason", "")
                }
            return None
        except (json.JSONDecodeError, KeyError):
            return None


# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒˆã‚¢ï¼ˆã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼‰
_session_store: Dict[str, SessionState] = {}


def get_session(session_id: str) -> Optional[SessionState]:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—"""
    return _session_store.get(session_id)


def create_session(task_id: str) -> SessionState:
    """æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆåŒã˜task_idã®å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯å‰Šé™¤ï¼‰"""
    # åŒã˜task_idã®å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
    sessions_to_delete = [
        sid for sid, s in _session_store.items()
        if s.task_id == task_id
    ]
    for sid in sessions_to_delete:
        del _session_store[sid]

    session = SessionState(
        session_id=str(uuid.uuid4()),
        task_id=task_id,
        phase=GenerationPhase.CONTEXT
    )
    _session_store[session.session_id] = session
    return session


def delete_session(session_id: str) -> bool:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤"""
    if session_id in _session_store:
        del _session_store[session_id]
        return True
    return False


def restore_session_from_db(hands_on: 'TaskHandsOn', task_id: str) -> Optional[SessionState]:
    """DBã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’å¾©å…ƒ"""
    if not hands_on or not hands_on.user_interactions:
        return None

    interactions = hands_on.user_interactions
    phase_str = interactions.get("phase", "CONTEXT")

    # ãƒ•ã‚§ãƒ¼ã‚ºã‚’å¾©å…ƒ
    try:
        phase = GenerationPhase(phase_str)
    except ValueError:
        phase = GenerationPhase.CONTEXT

    # å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—ã‚’å¾©å…ƒ
    steps_data = interactions.get("steps", [])
    implementation_steps = [
        ImplementationStep(
            step_number=s["step_number"],
            title=s["title"],
            description=s["description"],
            content=s.get("content", ""),
            is_completed=s.get("is_completed", False),
            user_feedback=s.get("user_feedback")
        )
        for s in steps_data
    ]

    # æ±ºå®šäº‹é …ã‚’å¾©å…ƒ
    decisions_data = interactions.get("decisions", [])
    decisions = [
        Decision(
            step_number=d["step_number"],
            description=d["description"],
            reason=d.get("reason", "")
        )
        for d in decisions_data
    ]

    # ä¿ç•™ä¸­ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¾©å…ƒ
    pending_input_data = interactions.get("pending_input")
    pending_input = None
    if pending_input_data:
        pending_input = InputPrompt(
            prompt_id=pending_input_data.get("prompt_id", ""),
            question=pending_input_data.get("question", ""),
            placeholder=pending_input_data.get("placeholder"),
            options=pending_input_data.get("options")
        )

    # user_choicesã‚’å¾©å…ƒ
    choices_data = interactions.get("choices", [])
    user_choices = {}
    for choice in choices_data:
        choice_id = choice.get("choice_id")
        if choice_id:
            user_choices[choice_id] = {
                "selected": choice.get("selected"),
                "user_note": choice.get("user_note")
            }

    # ç”Ÿæˆæ¸ˆã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å¾©å…ƒ
    generated_content = {
        "overview": hands_on.overview or "",
        "implementation": hands_on.implementation_steps or "",
        "verification": hands_on.verification or "",
        "context": hands_on.technical_context or ""
    }

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    session = SessionState(
        session_id=hands_on.session_id or str(uuid.uuid4()),
        task_id=task_id,
        phase=phase,
        generated_content=generated_content,
        user_choices=user_choices,
        user_inputs=interactions.get("inputs", {}),
        pending_input=pending_input,
        implementation_steps=implementation_steps,
        current_step_index=interactions.get("current_step", 0),
        decisions=decisions,
        pending_decision=interactions.get("pending_decision")
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒˆã‚¢ã«ç™»éŒ²
    _session_store[session.session_id] = session

    return session
